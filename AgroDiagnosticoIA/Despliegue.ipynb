{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvDOpcBmxe2_",
        "outputId": "0a7453ba-5886-45e0-b6c4-b3a31d37b245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-multipart in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYXhF7u8s1Xm",
        "outputId": "34c51e0b-189a-43ad-fc04-bc157af209f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.115.0)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\usuario\\appdata\\roaming\\python\\python310\\site-packages (1.5.8)\n",
            "Requirement already satisfied: pyngrok in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (7.2.0)\n",
            "Requirement already satisfied: uvicorn in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.30.6)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi) (0.38.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi) (4.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: click>=7.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\roaming\\python\\python310\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette<0.39.0,>=0.37.2->fastapi) (4.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\usuario\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5Y6f5t8giy"
      },
      "source": [
        "# Cargamos el modelo preentrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HMAjlXuz5y5n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_wzM6YBD57WV"
      },
      "outputs": [],
      "source": [
        "# Función para entrenar una red neuronal convolucional basada en el modelo ResNet\n",
        "\n",
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        # Cargar el modelo ResNet preentrenado\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Reemplazar la capa final de clasificación de ResNet para adaptarse al número de clases deseado\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Identity()  # Remover la capa final existente\n",
        "\n",
        "        # Definir las nuevas capas de clasificación\n",
        "        self.fc1 = nn.Linear(in_features, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(32, 4)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)  # Pasar por el ResNet sin la capa final\n",
        "        x = self.fc1(x)    # Pasar por la capa densa 1\n",
        "        x = self.relu(x)   # Aplicar ReLU\n",
        "        x = self.fc2(x)    # Pasar por la capa densa 2\n",
        "        x = self.softmax(x)  # Aplicar Softmax\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnaWzmdI53wt",
        "outputId": "3ff7324d-2869-4fdb-d607-a725a852c435"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_5852\\1379765419.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(ruta_checkpoint, map_location=device))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "model = ResNetModel().to(device)\n",
        "\n",
        "# Ruta al archivo checkpoint\n",
        "ruta_checkpoint = 'Modelo.pt'\n",
        "\n",
        "# Cargar el estado del modelo desde el checkpoint\n",
        "model.load_state_dict(torch.load(ruta_checkpoint, map_location=device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0_dMsiV8scy"
      },
      "source": [
        "# Usamos FastAPI para desplegar el modelo en internet a través de una interfaz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports y funciones necesarias para el despliegue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "okLoFBUHAUzc"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, Request, Form, File, UploadFile\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from fastapi.responses import RedirectResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import HTMLResponse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import io\n",
        "from fastapi.responses import StreamingResponse\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pl8GZdYw86c8"
      },
      "outputs": [],
      "source": [
        "# Definir las transformaciones de preprocesamiento\n",
        "def preprocess_image(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Cambia de BGR a RGB\n",
        "    image = image.astype(np.float32) / 255.0  # Normaliza a [0, 1]\n",
        "    tensor_image = torch.tensor(image).permute(2, 0, 1)  # Cambia la forma a CxHxW\n",
        "    tensor_image = tensor_image.unsqueeze(0)  # Añade la dimensión del batch\n",
        "    return tensor_image\n",
        "\n",
        "\n",
        "def resize_image(img):\n",
        "    # Convierte a un array NumPy desde el contenido de la imagen\n",
        "    nparr = np.frombuffer(img, np.uint8)\n",
        "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)  # Decodifica la imagen\n",
        "\n",
        "    if image is None:\n",
        "        raise ValueError(\"Error al decodificar la imagen.\")\n",
        "\n",
        "    # Verifica la forma de la imagen\n",
        "    shape = image.shape\n",
        "\n",
        "    # Redimensionar si no tiene el tamaño esperado\n",
        "    if shape[0] == 224 and shape[1] == 224:\n",
        "        return image\n",
        "    else:\n",
        "        dim = (224, 224)\n",
        "        resized = cv2.resize(image, dim)\n",
        "        return resized\n",
        "\n",
        "def predict(image_tensor):\n",
        "    with torch.no_grad():#desactivamos el calculo de gradientes\n",
        "        output = model(image_tensor)#hacer la inferencia\n",
        "        _, predicted = torch.max(output.data, 1)#obtener la clase predicha\n",
        "    return predicted.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9Z-HkvCzwIoQ"
      },
      "outputs": [],
      "source": [
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "# Montar archivos estáticos (CSS, imágenes, etc.)\n",
        "app.mount(\"/static/css\", StaticFiles(directory = \"templates/stylescss\"), name=\"css\")\n",
        "app.mount(\"/static/images\", StaticFiles(directory = \"templates/images\"), name=\"images\")\n",
        "app.mount(\"/static/js\", StaticFiles(directory = \"templates\"), name=\"js\")\n",
        "\n",
        "# Configurar las plantillas\n",
        "templates = Jinja2Templates(directory=\"templates/\")\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def landing_page(request: Request):\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"title\": \"Agro Diagnóstico IA\"})\n",
        "\n",
        "@app.get(\"/upload/\")\n",
        "async def upload_page(request: Request):\n",
        "    return templates.TemplateResponse(\"plataforma.html\", {\"request\": request})\n",
        "\n",
        "\n",
        "@app.post(\"/upload/\")\n",
        "async def handle_upload(request: Request, file: UploadFile = File(...)):\n",
        "    try:\n",
        "        content = await file.read()  # Lee el contenido del archivo\n",
        "        nparr = np.frombuffer(content, np.uint8)\n",
        "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if image is None:\n",
        "            raise ValueError(\"Error al decodificar la imagen.\")\n",
        "\n",
        "\n",
        "        # Redimensionamos la imagen\n",
        "        resized_image = cv2.resize(image, (224, 224))  # Cambia a image\n",
        "        preprocess = transforms.Compose([\n",
        "            transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "            ])\n",
        "        image_tensor = preprocess(resized_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)  # Agregar dimensión de lote y mover a dispositivo\n",
        "\n",
        "        # Preparamos la imagen para el modelo\n",
        "        #tensor_image = preprocess_image(resized_image)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output = model(image_tensor)  # Obtener la salida del modelo\n",
        "            print(f\"Salida del modelo: {output}\")  # Verifica la salida del modelo\n",
        "\n",
        "        # Obtener la clase predicha\n",
        "        class_names = [\"Cordana\", \"Saludable\", \"Pestalotiopsis\", \"Sigatoka\"]\n",
        "        _, prediction = torch.max(output, 1)  # Obtener la clase con mayor probabilidad\n",
        "        predicted_index = prediction.item()\n",
        "        predicted_class_name = class_names[predicted_index]  # Obtener el nombre de la clase\n",
        "\n",
        "        # Codificamos la imagen redimensionada para la respuesta\n",
        "        _, buffer = cv2.imencode('.jpg', resized_image)  # Codifica la imagen redimensionada en JPEG\n",
        "        byte_io = io.BytesIO(buffer)  # Crea objeto BytesIO para la imagen\n",
        "        return StreamingResponse(byte_io, media_type=\"image/jpeg\", headers={\"X-Predicted-Class\": predicted_class_name})  # Devuelve la imagen procesada\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")  # Muestra el error en la consola\n",
        "        return {\"error\": str(e)}  # Devuelve el error como respuesta JSON\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Despliegue de la aplicación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71hVW6X5lWzR"
      },
      "source": [
        "Ejecuta la siguiente celda para desplegar el modelo online y cargar las imágenes:\n",
        "Es necesario que todas las anteriores celdas hayan sido previamente ejecutadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVZZODF6tEm9",
        "outputId": "c1d5c3dc-f83d-4f65-edaf-7eacdb8c072c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [5852]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://9c7e-177-254-18-219.ngrok-free.app\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/css/home.css HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/logoAgroDiagnosticoIA.png HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/dos.PNG HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/imagendron1.png HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/tres.PNG HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/uno.PNG HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/imageninicio.PNG HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/iconADIA.ico HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /upload/ HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/css/plat.css HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/js/scrip.js HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/botonMenu.PNG HTTP/1.1\" 200 OK\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"GET /static/images/ImagePlaceHolder.jpg HTTP/1.1\" 200 OK\n",
            "Salida del modelo: tensor([[0.0111, 0.7510, 0.0191, 0.2188]])\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"POST /upload/ HTTP/1.1\" 200 OK\n",
            "Salida del modelo: tensor([[0.0045, 0.0014, 0.9792, 0.0150]])\n",
            "INFO:     2800:e2:1a00:8dad:b518:66f7:a4c2:4775:0 - \"POST /upload/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [5852]\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "auth_token = \"2mWWl9c2JDShbjp2DF10HlzqYPy_4QUs4RZMwQJoERjDM5HHp\"\n",
        "\n",
        "# Set the authtoken\n",
        "ngrok.set_auth_token(auth_token)\n",
        "\n",
        "# Connect to ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "\n",
        "# Print the public URL\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the uvicorn server\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRnZlAWPmswg"
      },
      "source": [
        "# PRUEBA DEL MODELO CARGADO LOCALMENTE SIN TENER EN CUENTA FAST API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wCDKVfxAdzL",
        "outputId": "2c4ac787-04b0-42fd-8ce6-c11e249ca8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "tensor([2])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_11068\\3309231191.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model1.load_state_dict(torch.load(PATH, map_location=device))\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "PATH = 'Modelo.pt'\n",
        "model1 = ResNetModel().to(device)\n",
        "model1.load_state_dict(torch.load(PATH, map_location=device))\n",
        "model1.eval()\n",
        "\n",
        "path = 'Imagenes_prueba/Test2.jpg' # Ruta imagen a predecir\n",
        "\n",
        "im = cv2.imread(path)\n",
        "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "result = resized_image = cv2.resize(im, (224, 224))\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "])\n",
        "\n",
        "img_tensor = preprocess(result)\n",
        "img_tensor = img_tensor.unsqueeze(0) # Add a batch dimension\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "with torch.no_grad():  # No need to compute gradients\n",
        "    output = model1(img_tensor)\n",
        "\n",
        "# The output will be raw scores (logits); apply softmax to get probabilities\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "\n",
        "# Get the predicted class index\n",
        "_, predicted_class = torch.max(output, 1)\n",
        "\n",
        "# plt.imshow(result)\n",
        "print(predicted_class)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
